{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4892323",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/metrics/ablation_final.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      9\u001b[39m plt.rcParams.update({\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfont.family\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mDejaVu Sans\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfigure.dpi\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m150\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maxes.spines.top\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maxes.spines.right\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     14\u001b[39m })\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# ── Load data ──────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresults/metrics/ablation_final.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     18\u001b[39m     raw = json.load(f)\n\u001b[32m     20\u001b[39m rows = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/rag-system-analysis/rag-research/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'results/metrics/ablation_final.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'figure.dpi': 150,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "# ── Load data ──────────────────────────────────────────────────\n",
    "with open('results/metrics/ablation_final.json') as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "rows = []\n",
    "for e in raw:\n",
    "    c = e['config']\n",
    "    fm = e.get('failure_mode_distribution', {})\n",
    "    rows.append({\n",
    "        'exp_id':      c['exp_id'],\n",
    "        'method':      c['retrieval_method'].upper(),\n",
    "        'chunk_size':  c['chunk_size'],\n",
    "        'overlap':     c['overlap'],\n",
    "        'top_k':       c['top_k'],\n",
    "        'faithfulness':      e['avg_faithfulness'],\n",
    "        'ctx_relevance':     e['avg_context_relevance'],\n",
    "        'ans_relevance':     e['avg_answer_relevance'],\n",
    "        'hallucination':     e['hallucination_rate'],\n",
    "        'abstention_rate':   e['honest_abstention_rate'],\n",
    "        'latency':           e['avg_latency'],\n",
    "        'correct':           fm.get('correct', 0),\n",
    "        'partial_context':   fm.get('partial_context', 0),\n",
    "        'honest_abstention': fm.get('honest_abstention', 0),\n",
    "        'total_queries':     e['total_queries'],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values('faithfulness', ascending=False).reset_index(drop=True)\n",
    "df['rank'] = df.index + 1\n",
    "df['label'] = df['exp_id'] + '\\n' + df['method'] + ' c' + df['chunk_size'].astype(str)\n",
    "df['config'] = df['method'] + '\\nchunk=' + df['chunk_size'].astype(str)\n",
    "\n",
    "COLORS = {'BM25': '#1F77B4', 'DENSE': '#2CA02C', 'HYBRID': '#FF7F0E'}\n",
    "df['color'] = df['method'].map(COLORS)\n",
    "\n",
    "print(df[['exp_id','method','chunk_size','faithfulness','ctx_relevance','latency','rank']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa4c96",
   "metadata": {},
   "source": [
    "## Figure 1 — Leaderboard: Faithfulness & Context Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Figure 1 — Experiment Leaderboard', fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "metrics = [('faithfulness', 'Faithfulness Score'), ('ctx_relevance', 'Context Relevance Score')]\n",
    "for ax, (metric, title) in zip(axes, metrics):\n",
    "    bars = ax.barh(df['label'][::-1], df[metric][::-1],\n",
    "                   color=df['color'][::-1], edgecolor='white', height=0.6)\n",
    "    for bar, val in zip(bars, df[metric][::-1]):\n",
    "        ax.text(val + 0.005, bar.get_y() + bar.get_height()/2,\n",
    "                f'{val:.3f}', va='center', fontsize=9, fontweight='bold')\n",
    "    ax.set_xlim(0, 1.12)\n",
    "    ax.set_xlabel(title, fontsize=11)\n",
    "    ax.axvline(x=df[metric].mean(), color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.text(df[metric].mean()+0.005, -0.5, f'avg={df[metric].mean():.3f}',\n",
    "            color='gray', fontsize=8)\n",
    "\n",
    "legend = [mpatches.Patch(color=v, label=k) for k, v in COLORS.items()]\n",
    "axes[0].legend(handles=legend, loc='lower right', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig1_leaderboard.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: results/fig1_leaderboard.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158080b9",
   "metadata": {},
   "source": [
    "## Figure 2 — Chunk Size vs Metrics (per Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Figure 2 — Chunk Size Effect per Retrieval Method', fontsize=14, fontweight='bold')\n",
    "\n",
    "metrics3 = [('faithfulness','Faithfulness'), ('ctx_relevance','Context Relevance'), ('latency','Latency (s)')]\n",
    "chunk_sizes = [256, 512]\n",
    "\n",
    "for ax, (metric, ylabel) in zip(axes, metrics3):\n",
    "    for method, color in COLORS.items():\n",
    "        sub = df[df['method'] == method].sort_values('chunk_size')\n",
    "        if len(sub) == 2:\n",
    "            ax.plot(sub['chunk_size'], sub[metric], 'o-', color=color,\n",
    "                    label=method, linewidth=2, markersize=8)\n",
    "            for _, row in sub.iterrows():\n",
    "                ax.annotate(f\"{row[metric]:.2f}\",\n",
    "                            (row['chunk_size'], row[metric]),\n",
    "                            textcoords='offset points', xytext=(5, 5), fontsize=8)\n",
    "    ax.set_xticks(chunk_sizes)\n",
    "    ax.set_xlabel('Chunk Size', fontsize=11)\n",
    "    ax.set_ylabel(ylabel, fontsize=11)\n",
    "    ax.set_title(ylabel, fontsize=11)\n",
    "    ax.legend(fontsize=9)\n",
    "    if metric != 'latency':\n",
    "        ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig2_chunksize_effect.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: results/fig2_chunksize_effect.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f23a2",
   "metadata": {},
   "source": [
    "## Figure 3 — Heatmap: All Metrics per Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d67274",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "fig.suptitle('Figure 3 — Metrics Heatmap (All Experiments)', fontsize=14, fontweight='bold')\n",
    "\n",
    "hm_cols = ['faithfulness', 'ctx_relevance', 'ans_relevance', 'hallucination', 'abstention_rate']\n",
    "hm_labels = ['Faithfulness', 'Context\\nRelevance', 'Answer\\nRelevance', 'Hallucination\\nRate', 'Abstention\\nRate']\n",
    "hm_df = df.set_index('exp_id')[hm_cols]\n",
    "\n",
    "# Custom colormap: green=good for faithfulness/ctx/ans, red=bad for hallucination\n",
    "sns.heatmap(hm_df, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "            linewidths=0.5, linecolor='white',\n",
    "            ax=ax, vmin=0, vmax=1,\n",
    "            xticklabels=hm_labels, annot_kws={'size': 11, 'weight': 'bold'})\n",
    "\n",
    "ax.set_ylabel('')\n",
    "# Annotate rank\n",
    "for i, row in df.iterrows():\n",
    "    ax.text(-0.6, i + 0.5, f'#{row[\"rank\"]}', va='center', ha='center',\n",
    "            fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig3_heatmap.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: results/fig3_heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9e4cd",
   "metadata": {},
   "source": [
    "## Figure 4 — Failure Mode Distribution (Stacked Bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bffee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "fig.suptitle('Figure 4 — Failure Mode Distribution per Experiment', fontsize=14, fontweight='bold')\n",
    "\n",
    "x = np.arange(len(df))\n",
    "w = 0.5\n",
    "total = df['total_queries']\n",
    "\n",
    "p1 = ax.bar(x, df['correct']/total*100,       w, label='Correct',           color='#2ECC71', edgecolor='white')\n",
    "p2 = ax.bar(x, df['partial_context']/total*100, w, label='Partial Context',  color='#F39C12', edgecolor='white',\n",
    "            bottom=df['correct']/total*100)\n",
    "p3 = ax.bar(x, df['honest_abstention']/total*100, w, label='Honest Abstention', color='#95A5A6', edgecolor='white',\n",
    "            bottom=(df['correct']+df['partial_context'])/total*100)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df['config'], fontsize=9)\n",
    "ax.set_ylabel('Percentage of Queries (%)', fontsize=11)\n",
    "ax.set_ylim(0, 115)\n",
    "ax.legend(fontsize=10, loc='upper right')\n",
    "ax.axhline(y=100, color='black', linestyle=':', alpha=0.3)\n",
    "\n",
    "# Value labels on bars\n",
    "for bar in [p1, p2, p3]:\n",
    "    for rect in bar:\n",
    "        h = rect.get_height()\n",
    "        if h > 5:\n",
    "            ax.text(rect.get_x() + rect.get_width()/2., rect.get_y() + h/2,\n",
    "                    f'{h:.0f}%', ha='center', va='center', fontsize=8, fontweight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig4_failure_modes.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: results/fig4_failure_modes.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efec4212",
   "metadata": {},
   "source": [
    "## Figure 5 — Quality vs Latency Trade-off (Bubble Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39349b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m fig, ax = \u001b[43mplt\u001b[49m.subplots(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      2\u001b[39m fig.suptitle(\u001b[33m'\u001b[39m\u001b[33mFigure 5 — Quality vs Latency Trade-off\u001b[39m\u001b[33m'\u001b[39m, fontsize=\u001b[32m14\u001b[39m, fontweight=\u001b[33m'\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows():\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fig.suptitle('Figure 5 — Quality vs Latency Trade-off', fontsize=14, fontweight='bold')\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    size = (row['ctx_relevance'] + 0.1) * 800\n",
    "    ax.scatter(row['latency'], row['faithfulness'],\n",
    "               s=size, color=COLORS[row['method']],\n",
    "               alpha=0.75, edgecolors='white', linewidth=1.5, zorder=3)\n",
    "    ax.annotate(f\"{row['exp_id']}\\n{row['method']} c{row['chunk_size']}\",\n",
    "                (row['latency'], row['faithfulness']),\n",
    "                xytext=(8, 4), textcoords='offset points', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Average Latency (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Faithfulness Score', fontsize=12)\n",
    "ax.set_ylim(0.5, 1.1)\n",
    "ax.axhline(y=df['faithfulness'].mean(), color='gray', linestyle='--', alpha=0.4, label=f'avg faith={df[\"faithfulness\"].mean():.3f}')\n",
    "ax.axvline(x=df['latency'].mean(), color='gray', linestyle=':', alpha=0.4, label=f'avg lat={df[\"latency\"].mean():.0f}s')\n",
    "\n",
    "legend = [mpatches.Patch(color=v, label=k) for k, v in COLORS.items()]\n",
    "ax.legend(handles=legend + [plt.Line2D([0],[0],color='gray',linestyle='--',label=f'avg faith={df[\"faithfulness\"].mean():.3f}'),\n",
    "                              plt.Line2D([0],[0],color='gray',linestyle=':',label=f'avg lat={df[\"latency\"].mean():.0f}s')],\n",
    "          fontsize=9)\n",
    "ax.text(0.02, 0.97, 'Bubble size = Context Relevance', transform=ax.transAxes,\n",
    "        fontsize=8, color='gray', va='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig5_quality_latency.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: results/fig5_quality_latency.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43780185",
   "metadata": {},
   "source": [
    "## Figure 6 — Method Comparison (Radar / Spider Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef914db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "# Average per method\n",
    "method_avg = df.groupby('method')[['faithfulness','ctx_relevance','ans_relevance','abstention_rate']].mean()\n",
    "method_avg['correct_rate'] = df.groupby('method').apply(lambda x: (x['correct']/x['total_queries']).mean())\n",
    "\n",
    "categories = ['Faithfulness', 'Ctx Relevance', 'Ans Relevance', 'Correct Rate', 'Low Abstention']\n",
    "methods = method_avg.index.tolist()\n",
    "\n",
    "values_dict = {}\n",
    "for m in methods:\n",
    "    r = method_avg.loc[m]\n",
    "    values_dict[m] = [\n",
    "        r['faithfulness'],\n",
    "        r['ctx_relevance'],\n",
    "        r['ans_relevance'],\n",
    "        r['correct_rate'],\n",
    "        1 - r['abstention_rate'],   # invert: lower abstention = better\n",
    "    ]\n",
    "\n",
    "N = len(categories)\n",
    "angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7), subplot_kw=dict(polar=True))\n",
    "fig.suptitle('Figure 6 — Method Comparison (Radar)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for method in methods:\n",
    "    vals = values_dict[method] + values_dict[method][:1]\n",
    "    ax.plot(angles, vals, 'o-', linewidth=2, label=method, color=COLORS[method])\n",
    "    ax.fill(angles, vals, alpha=0.1, color=COLORS[method])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=10)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.25, 0.5, 0.75, 1.0])\n",
    "ax.set_yticklabels(['0.25','0.50','0.75','1.00'], fontsize=7, color='gray')\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/fig6_radar.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: results/fig6_radar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee20776",
   "metadata": {},
   "source": [
    "## Summary Table — Final Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96ed29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "  FINAL ABLATION SUMMARY\n",
      "===========================================================================\n",
      "Rank  Exp      Method   Chunk     Faith   CtxRel   AnsRel   Lat(s)\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mRank\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<5\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mExp\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mMethod\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mChunk\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mFaith\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mCtxRel\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mAnsRel\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33mLat(s)\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m * \u001b[32m75\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, r \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m.iterrows():\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[33m\"\u001b[39m\u001b[33mrank\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<5\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[33m\"\u001b[39m\u001b[33mexp_id\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[33m\"\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[33m\"\u001b[39m\u001b[33mchunk_size\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[33m\"\u001b[39m\u001b[33mfaithfulness\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>8.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[33m\"\u001b[39m\u001b[33mctx_relevance\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>8.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[33m\"\u001b[39m\u001b[33mans_relevance\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>8.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[33m\"\u001b[39m\u001b[33mlatency\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>8.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m * \u001b[32m75\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print('=' * 75)\n",
    "print('  FINAL ABLATION SUMMARY')\n",
    "print('=' * 75)\n",
    "print(f'{\"Rank\":<5} {\"Exp\":<8} {\"Method\":<8} {\"Chunk\":<6} {\"Faith\":>8} {\"CtxRel\":>8} {\"AnsRel\":>8} {\"Lat(s)\":>8}')\n",
    "print('-' * 75)\n",
    "for _, r in df.iterrows():\n",
    "    print(f'{r[\"rank\"]:<5} {r[\"exp_id\"]:<8} {r[\"method\"]:<8} {r[\"chunk_size\"]:<6} {r[\"faithfulness\"]:>8.3f} {r[\"ctx_relevance\"]:>8.3f} {r[\"ans_relevance\"]:>8.3f} {r[\"latency\"]:>8.1f}')\n",
    "print('=' * 75)\n",
    "print(f'\\nWINNER: {df.iloc[0][\"exp_id\"]} — {df.iloc[0][\"method\"]} chunk={df.iloc[0][\"chunk_size\"]}')\n",
    "print(f'ALL HALLUCINATION RATES: 0.000 (zero hallucinations across all experiments)')\n",
    "print(f'\\nKey insight: BM25 chunk=256 wins on faithfulness (1.000)')\n",
    "print(f'Key insight: Hybrid chunk=256 underperforms — RRF hurts with small chunks')\n",
    "print(f'Key insight: All methods plateau at ans_relevance=0.800 — generation stable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc726258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
