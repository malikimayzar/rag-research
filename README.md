# ğŸ” RAG System Analysis
### Ablation Study on Chunking Strategies and Retrieval Methods

> **Maliki Mayzar** Â· February 2025  
> A complete Retrieval-Augmented Generation (RAG) pipeline built and evaluated from scratch across 8 development phases.

---

## ğŸ“Š Key Results (TL;DR)

| Rank | Method | Chunk | Faithfulness | Hallucination | Latency |
|------|--------|-------|-------------|--------------|---------|
| ğŸ¥‡ 1 | BM25 | 256 | **1.000** | **0%** | 199s |
| ğŸ¥ˆ 2 | Hybrid RRF | 512 | **1.000** | **0%** | 199s |
| ğŸ¥‰ 3 | Dense | 512 | 0.933 | **0%** | 316s |
| 4 | Dense | 256 | 0.917 | **0%** | 233s |
| 5 | BM25 | 512 | 0.833 | **0%** | 328s |
| 6 | Hybrid RRF | 256 | 0.633 | **0%** | 211s |

**Zero hallucinations across all 18 evaluated queries.**

---

## ğŸ—‚ï¸ Project Structure

```
rag-research/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # 7 ArXiv PDFs (Tier 1â€“3)
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ index_bm25/         # BM25 inverted index
â”‚   â”‚   â””â”€â”€ index_minilm/       # FAISS vector index
â”‚   â””â”€â”€ adversarial/            # Adversarial query set
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/              # PDF parsing, chunking
â”‚   â”œâ”€â”€ retrieval/              # Dense + sparse retrieval
â”‚   â”œâ”€â”€ generation/             # Mistral LLM interface
â”‚   â””â”€â”€ evaluation/             # Metrics & scoring
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ reranking/              # Cross-encoder reranking
â”‚   â””â”€â”€ ablation_runner.py      # Main ablation script
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb          # Visualization notebook
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/                # 6 visualization plots
â”‚   â”œâ”€â”€ logs/                   # ablation_full.log
â”‚   â”œâ”€â”€ metrics/                # JSON results
â”‚   â””â”€â”€ failure_cases/          # Failure analysis
â””â”€â”€ reports/
    â”œâ”€â”€ README.md               # This file
    â”œâ”€â”€ FINAL_REPORT.md         # Full mini-paper
    â””â”€â”€ RAG_Final_Report.pdf    # PDF version
```

---

## ğŸ—ï¸ System Architecture

```
PDF Documents
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Chunker   â”‚  fixed_size strategy, configurable chunk_size + overlap
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
  â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚FAISS â”‚  â”‚ BM25 â”‚  all-MiniLM-L6-v2 (384-dim) + JSON inverted index
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜
  â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RRF Fusion â”‚  Reciprocal Rank Fusion (k=60)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Mistral    â”‚  Local LLM via Ollama, context-grounded prompting
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â–¼
    Answer
```

---

## ğŸ“š Dataset

7 ArXiv papers organized in 3 tiers:

| Tier | Paper ID | Topic |
|------|----------|-------|
| 1 | 2005.11401 | RAG (Lewis et al.) |
| 1 | 2312.10997 | Advanced RAG techniques |
| 1 | tier1_test_intro | Custom intro document |
| 2 | 2210.11610 | Retrieval methods |
| 2 | 2212.10560 | Dense retrieval |
| 3 | 1706.03762 | Attention Is All You Need |
| 3 | 2307.09288 | LLM survey |

---

## ğŸš€ Quick Start

### Prerequisites
```bash
# Python 3.10+
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Ollama + Mistral (local LLM)
ollama pull mistral
```

### Run Pipeline
```bash
# Step 1 â€” Ingest documents
python src/ingestion/document_loader.py

# Step 2 â€” Build indexes
python src/ingestion/chunker.py

# Step 3 â€” Quick ablation (3 experiments)
python experiments/ablation_runner.py

# Step 4 â€” Full ablation (6 experiments, ~4 hours)
python experiments/ablation_runner.py --full

# Step 5 â€” Visualize results
python visualize.py
```

### View Results
```bash
# Quick summary
python3 -c "
import json
data = json.load(open('results/metrics/ablation_final.json'))
for e in sorted(data, key=lambda x: -x['avg_faithfulness']):
    print(f\"{e['config']['exp_id']} | {e['config']['retrieval_method']:7} | \
chunk={e['config']['chunk_size']} | F:{e['avg_faithfulness']:.3f} \
C:{e['avg_context_relevance']:.3f}\")
"
```

---

## ğŸ“ˆ Visualizations

All figures are in `results/figures/`:

| Figure | Description |
|--------|-------------|
| `fig1_leaderboard.png` | Faithfulness & context relevance ranking |
| `fig2_chunksize.png` | Chunk size 256 vs 512 per method |
| `fig3_heatmap.png` | Full metrics heatmap |
| `fig4_failure_modes.png` | Correct / partial / abstention breakdown |
| `fig5_quality_latency.png` | Quality vs speed trade-off bubble chart |
| `fig6_radar.png` | Method comparison radar chart |

---

## ğŸ”¬ Ablation Configuration

6 experiments Ã— 3 queries = **18 total evaluations**

| Exp | Chunk | Overlap | Method | Total Runtime |
|-----|-------|---------|--------|--------------|
| exp_001 | 512 | 64 | Dense | â€” |
| exp_002 | 512 | 64 | BM25 | â€” |
| exp_003 | 512 | 64 | Hybrid RRF | â€” |
| exp_004 | 256 | 0 | Dense | â€” |
| exp_005 | 256 | 0 | BM25 | â€” |
| exp_006 | 256 | 0 | Hybrid RRF | â€” |
| **Total** | | | | **241.2 minutes** |

---

## ğŸ’¡ Key Findings

### 1. Zero Hallucinations
Across all 18 queries and 6 configurations, hallucination rate = **0.000**. Context-grounding prompting with honest abstention works.

### 2. BM25 Wins with Small Chunks
BM25 + chunk=256 achieves perfect faithfulness (1.000). For precise technical queries with distinctive keywords, exact-match scoring outperforms semantic retrieval.

### 3. Hybrid RRF Requires Large Chunks
Hybrid RRF with chunk=512+overlap=64 â†’ faithfulness=1.000.  
Hybrid RRF with chunk=256+no overlap â†’ faithfulness=0.633 (worst).

> **Rule of thumb: Do not use Hybrid RRF with chunks smaller than ~384 tokens.**

### 4. Honest Abstention â‰  Failure
56% of queries triggered "honest abstention" â€” the LLM correctly declining to answer because the fact wasn't in the corpus. This is the **desired behavior** for a trustworthy system.

### 5. Answer Relevance is Always 0.800
Generation quality is stable regardless of retrieval config. The bottleneck is retrieval, not generation.

---

## ğŸ› ï¸ Development Phases

| Phase | Description | Status |
|-------|-------------|--------|
| 0 | Environment setup | âœ… |
| 1 | Ingestion + Chunking | âœ… |
| 2 | Dense + Sparse Retrieval | âœ… |
| 3 | Hybrid RRF | âœ… |
| 4 | LLM Generation (Mistral) | âœ… |
| 5 | Evaluation Pipeline | âœ… |
| 6 | Real Dataset (7 ArXiv papers) | âœ… |
| 7 | Quick Ablation (3 experiments) | âœ… |
| 7b | Full Ablation (6 experiments) | âœ… |
| 8 | Visualization Notebook | âœ… |
| 9 | Error Analysis | âœ… |
| 10 | Final Report | âœ… |

---

## ğŸ“ Failure Mode Analysis

| Mode | Count | % | Meaning |
|------|-------|---|---------|
| `correct` | 6 | 33% | Perfect retrieval + generation |
| `honest_abstention` | 10 | 56% | Answer not in corpus (correct behavior) |
| `partial_context` | 2 | 11% | Chunking artifact (truncated sentence retrieved) |
| `hallucination` | 0 | 0% | Never occurred |

Root causes:
- **partial_context** â†’ Fixed-size chunking splits sentences mid-word. Fix: sentence-boundary-aware chunking.
- **honest_abstention** â†’ Out-of-corpus queries. Not a bug â€” this is the system working correctly.

---

## ğŸ”® Future Work

- [ ] Sentence-boundary-aware chunking to eliminate partial_context failures
- [ ] Cross-encoder reranking (`experiments/reranking/`) to push ctx_relevance above 0.667
- [ ] Ground truth QA pairs for precision/recall evaluation
- [ ] Larger query set (10+ per experiment) for statistical significance
- [ ] Test with larger LLMs (Llama 3, Mixtral)

---

## ğŸ“„ Report

Full analysis available in:
- [`reports/FINAL_REPORT.md`](reports/FINAL_REPORT.md) â€” Mini-paper format (Abstract â†’ Conclusion)
- [`reports/RAG_Final_Report.pdf`](reports/RAG_Final_Report.pdf) â€” PDF version

---

## âš™ï¸ Tech Stack

| Component | Tool |
|-----------|------|
| Language | Python 3.12 |
| Vector Store | FAISS |
| Sparse Retrieval | BM25 (custom) |
| Embedding Model | all-MiniLM-L6-v2 |
| LLM | Mistral (Ollama) |
| Visualization | matplotlib, seaborn |
| PDF Generation | reportlab |
| Environment | WSL2 Ubuntu + venv |

---

*RAG Research Project Â· February 2025*