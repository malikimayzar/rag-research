# ğŸ” RAG System Analysis
### Ablation Study on Chunking Strategies and Retrieval Methods

> **Maliki Mayzar** Â· February 2025  
> Complete Retrieval-Augmented Generation (RAG) pipeline built from scratch across 8 development phases.

---

## ğŸ“Š Key Results (TL;DR)

| Rank | Method | Chunk | Faithfulness | Hallucination | Latency |
|------|--------|-------|-------------|--------------|---------|
| ğŸ¥‡ 1 | BM25 | 256 | **1.000** | **0%** | 199s |
| ğŸ¥ˆ 2 | Hybrid RRF | 512 | **1.000** | **0%** | 199s |
| ğŸ¥‰ 3 | Dense | 512 | 0.933 | **0%** | 316s |
| 4 | Dense | 256 | 0.917 | **0%** | 233s |
| 5 | BM25 | 512 | 0.833 | **0%** | 328s |
| 6 | Hybrid RRF | 256 | 0.633 | **0%** | 211s |

**Zero hallucinations across all 18 evaluated queries.**

---

## ğŸ—‚ï¸ Project Structure

```
rag-research/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # 7 ArXiv PDFs + TXT
â”‚   â”‚   â”œâ”€â”€ tier1_2005_11401.pdf      # RAG (Lewis et al.)
â”‚   â”‚   â”œâ”€â”€ tier1_2312_10997.pdf      # Advanced RAG
â”‚   â”‚   â”œâ”€â”€ tier1_test_intro.txt      # Custom intro doc
â”‚   â”‚   â”œâ”€â”€ tier2_2210_11610.pdf      # Retrieval methods
â”‚   â”‚   â”œâ”€â”€ tier2_2212_10560.pdf      # Dense retrieval
â”‚   â”‚   â”œâ”€â”€ tier3_1706_03762.pdf      # Attention Is All You Need
â”‚   â”‚   â””â”€â”€ tier3_2307_09288.pdf      # LLM survey
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ documents.json            # Parsed documents
â”‚   â”‚   â”œâ”€â”€ chunks_512_64.json        # Chunks (size=512, overlap=64)
â”‚   â”‚   â”œâ”€â”€ dataset_meta.json         # Dataset metadata
â”‚   â”‚   â”œâ”€â”€ index_bm25/
â”‚   â”‚   â”‚   â”œâ”€â”€ bm25_chunks.json      # BM25 chunk index
â”‚   â”‚   â”‚   â””â”€â”€ bm25.pkl              # BM25 serialized index
â”‚   â”‚   â””â”€â”€ index_minilm/             # FAISS vector index
â”‚   â””â”€â”€ adversarial/
â”‚       â””â”€â”€ adversarial_queries.json  # Adversarial test queries
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ document_loader.py        # PDF/TXT parsing
â”‚   â”‚   â”œâ”€â”€ chunker.py                # Fixed-size chunking
â”‚   â”‚   â””â”€â”€ dataset_builder.py        # Dataset construction
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ embedder.py               # all-MiniLM-L6-v2 encoding
â”‚   â”‚   â”œâ”€â”€ bm25_retriever.py         # BM25 sparse retrieval
â”‚   â”‚   â””â”€â”€ hybrid_retriever.py       # RRF fusion
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â””â”€â”€ generator.py              # Mistral LLM interface
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ evaluator.py              # Metrics: faithfulness, ctx, ans
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ ablation_runner.py            # Main ablation script
â”‚   â”œâ”€â”€ chunking/                     # Chunking experiments
â”‚   â”œâ”€â”€ embedding/                    # Embedding experiments
â”‚   â”œâ”€â”€ hybrid/                       # Hybrid retrieval experiments
â”‚   â””â”€â”€ reranking/                    # Cross-encoder reranking
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb                # Visualization notebook
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/                      # 6 visualization plots
â”‚   â”‚   â”œâ”€â”€ fig1_leaderboard.png
â”‚   â”‚   â”œâ”€â”€ fig2_chunksize.png
â”‚   â”‚   â”œâ”€â”€ fig3_heatmap.png
â”‚   â”‚   â”œâ”€â”€ fig4_failure_modes.png
â”‚   â”‚   â”œâ”€â”€ fig5_quality_latency.png
â”‚   â”‚   â””â”€â”€ fig6_radar.png
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ ablation_final.json       # Final 6-experiment results
â”‚   â”‚   â”œâ”€â”€ ablation_incremental.json # Per-experiment incremental
â”‚   â”‚   â””â”€â”€ evaluation_results.json   # Phase 5â€“6 eval results
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â”œâ”€â”€ ablation_full.log         # Full ablation run log
â”‚   â”‚   â”œâ”€â”€ generation_test.json      # Generation test results
â”‚   â”‚   â””â”€â”€ hybrid_search_test.json   # Hybrid search test
â”‚   â””â”€â”€ failure_cases/
â”‚       â””â”€â”€ failure_analysis.json     # Failure mode details
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ FINAL_REPORT.md               # Mini-paper (Abstractâ†’Conclusion)
â”‚   â””â”€â”€ RAG_Final_Report.pdf          # PDF version
â”‚
â”œâ”€â”€ requirements/
â”‚   â”œâ”€â”€ base.txt                      # Core dependencies
â”‚   â”œâ”€â”€ llm.txt                       # LLM dependencies
â”‚   â”œâ”€â”€ api.txt                       # API dependencies
â”‚   â”œâ”€â”€ dev.txt                       # Dev tools
â”‚   â””â”€â”€ research.txt                  # Research tools
â”‚
â”œâ”€â”€ visualize.py                      # Visualization script
â”œâ”€â”€ requirements.txt                  # Main requirements
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ—ï¸ System Architecture

```
PDF/TXT Documents
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ document_loader  â”‚  PDF parsing + text extraction
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   chunker.py     â”‚  fixed_size strategy
â”‚  chunk=256/512   â”‚  overlap=0/64
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FAISS  â”‚ â”‚  BM25  â”‚  embedder.py + bm25_retriever.py
â”‚(dense) â”‚ â”‚(sparse)â”‚  all-MiniLM-L6-v2 (384-dim)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ hybrid_retriever â”‚  Reciprocal Rank Fusion (k=60)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  generator.py    â”‚  Mistral via Ollama (local)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  evaluator.py    â”‚  Faithfulness Â· Context Rel Â· Answer Rel
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites
```bash
# Python 3.12
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Ollama + Mistral (local LLM)
ollama pull mistral
```

### Run Pipeline
```bash
# Step 1 â€” Ingest & parse documents
python src/ingestion/document_loader.py

# Step 2 â€” Build chunks + indexes
python src/ingestion/chunker.py

# Step 3 â€” Quick ablation (3 experiments)
python experiments/ablation_runner.py

# Step 4 â€” Full ablation (6 experiments, ~4 hours)
python experiments/ablation_runner.py --full

# Step 5 â€” Generate visualizations
python visualize.py
```

### View Results Summary
```bash
python3 -c "
import json
data = json.load(open('results/metrics/ablation_final.json'))
for e in sorted(data, key=lambda x: -x['avg_faithfulness']):
    c = e['config']
    print(f\"{c['exp_id']} | {c['retrieval_method']:7} | chunk={c['chunk_size']} | \
F:{e['avg_faithfulness']:.3f} C:{e['avg_context_relevance']:.3f} | {e['avg_latency']:.1f}s\")
"
```

---

## ğŸ“ˆ Visualizations

| Figure | Description |
|--------|-------------|
| `fig1_leaderboard.png` | Faithfulness & context relevance ranking |
| `fig2_chunksize.png` | Chunk size 256 vs 512 per method |
| `fig3_heatmap.png` | Full metrics heatmap across all experiments |
| `fig4_failure_modes.png` | Correct / partial / abstention breakdown |
| `fig5_quality_latency.png` | Quality vs speed trade-off bubble chart |
| `fig6_radar.png` | Method comparison radar chart |

---

## ğŸ”¬ Ablation Study

6 experiments Ã— 3 queries = **18 total evaluations** Â· Runtime: **241.2 minutes**

| Exp | Chunk | Overlap | Method | Faithfulness | Ctx Rel | Latency |
|-----|-------|---------|--------|-------------|---------|---------|
| exp_001 | 512 | 64 | Dense | 0.933 | 0.667 | 316s |
| exp_002 | 512 | 64 | BM25 | 0.833 | 0.500 | 328s |
| exp_003 | 512 | 64 | Hybrid RRF | 1.000 | 0.667 | 199s |
| exp_004 | 256 | 0 | Dense | 0.917 | 0.667 | 233s |
| exp_005 | 256 | 0 | **BM25** | **1.000** | 0.667 | 199s |
| exp_006 | 256 | 0 | Hybrid RRF | 0.633 | 0.500 | 211s |

---

## ğŸ’¡ Key Findings

**1. Zero Hallucinations**  
Hallucination rate = 0.000 across all 18 queries. Context-grounding prompting + honest abstention works.

**2. BM25 Wins with Small Chunks**  
BM25 + chunk=256 â†’ faithfulness 1.000. Exact-match scoring excels on precise technical queries with distinctive keywords.

**3. Hybrid RRF Needs Large Chunks**  
chunk=512 + overlap=64 â†’ faithfulness 1.000. chunk=256 + no overlap â†’ faithfulness 0.633 (worst).
> âš ï¸ Do not use Hybrid RRF with chunks smaller than ~384 tokens on technical corpora.

**4. Honest Abstention â‰  Failure**  
56% of queries triggered abstention â€” LLM correctly declining when answer isn't in corpus. This is desired behavior for a trustworthy system.

**5. Generation is Stable**  
Answer relevance = 0.800 uniformly across all configs. The bottleneck is retrieval, not generation.

---

## ğŸ› ï¸ Development Phases

| Phase | Description | Status |
|-------|-------------|--------|
| 0 | Environment setup | âœ… |
| 1 | Ingestion + Chunking (`src/ingestion/`) | âœ… |
| 2 | Dense + Sparse Retrieval (`src/retrieval/`) | âœ… |
| 3 | Hybrid RRF (`hybrid_retriever.py`) | âœ… |
| 4 | LLM Generation (`src/generation/`) | âœ… |
| 5 | Evaluation Pipeline (`src/evaluation/`) | âœ… |
| 6 | Real Dataset (7 ArXiv papers) | âœ… |
| 7 | Quick Ablation (3 experiments) | âœ… |
| 7b | Full Ablation (6 experiments, 241 min) | âœ… |
| 8 | Visualization (`visualize.py`, `notebooks/`) | âœ… |
| 9 | Error Analysis (`results/failure_cases/`) | âœ… |
| 10 | Final Report (`reports/`) | âœ… |

---

## ğŸ“ Failure Mode Analysis

| Mode | Count | % | Meaning |
|------|-------|---|---------|
| `correct` | 6 | 33% | Perfect retrieval + generation |
| `honest_abstention` | 10 | 56% | Answer not in corpus (correct behavior âœ…) |
| `partial_context` | 2 | 11% | Truncated chunk retrieved (chunking artifact) |
| `hallucination` | 0 | 0% | Never occurred ğŸ‰ |

---

## ğŸ”® Future Work

- [ ] Sentence-boundary-aware chunking â†’ eliminate `partial_context` failures
- [ ] Cross-encoder reranking (`experiments/reranking/`) â†’ push ctx_relevance above 0.667
- [ ] Ground truth QA pairs â†’ enable precision/recall metrics
- [ ] Larger query set (10+ per experiment) â†’ statistical significance
- [ ] Test with Llama 3 / Mixtral â†’ compare hallucination rates

---

## âš™ï¸ Tech Stack

| Component | Tool |
|-----------|------|
| Language | Python 3.12 |
| Vector Store | FAISS |
| Sparse Retrieval | BM25 (custom) |
| Embedding | all-MiniLM-L6-v2 (384-dim) |
| LLM | Mistral (Ollama, local) |
| Visualization | matplotlib, seaborn |
| Environment | WSL2 Ubuntu + venv |

---

## ğŸ“„ Full Report

- [`reports/FINAL_REPORT.md`](reports/FINAL_REPORT.md) â€” Mini-paper format (Abstract â†’ Conclusion)
- [`reports/RAG_Final_Report.pdf`](reports/RAG_Final_Report.pdf) â€” PDF version

---

*RAG Research Project Â· February 2025 Â· [@malikimayzar](https://github.com/malikimayzar)*
