[
  {
    "query": "what is chunking in RAG?",
    "dense": [
      {
        "chunk_id": "tier1_test_intro_c0000",
        "doc_id": "tier1_test_intro",
        "text": "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with language model generation. Instead of relying solely on parametric knowledge stored in model weights, RAG systems retrieve relevant documents from an external corpus and use them as context for generation.\n\nThe core components of a RAG system include a document corpus, an embedding model for semantic search, a vector database for efficient retrieval, and a language model for answer generation. Each component introdu",
        "start_char": 0,
        "end_char": 512,
        "tier": 1,
        "chunk_index": 0,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.29658234119415283,
        "retrieval_rank": 1,
        "retrieval_method": "dense"
      },
      {
        "chunk_id": "tier1_test_intro_c0002",
        "doc_id": "tier1_test_intro",
        "text": "e.\n\nHybrid retrieval combines sparse methods like BM25 with dense embedding-based search. BM25 excels at exact keyword matching while dense retrieval captures semantic similarity. The combination typically outperforms either method alone, particularly for technical documents.\n\nEvaluation of RAG systems requires metrics beyond simple accuracy. Faithfulness measures how well the generated answer reflects retrieved context. Context precision measures the relevance of retrieved chunks. Answer relevance measures",
        "start_char": 896,
        "end_char": 1408,
        "tier": 1,
        "chunk_index": 2,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.25860241055488586,
        "retrieval_rank": 2,
        "retrieval_method": "dense"
      },
      {
        "chunk_id": "tier1_test_intro_c0001",
        "doc_id": "tier1_test_intro",
        "text": "d a language model for answer generation. Each component introduces potential failure points that must be analyzed systematically.\n\nChunking is the process of splitting long documents into smaller segments suitable for embedding and retrieval. The choice of chunk size and overlap strategy significantly affects retrieval quality. Smaller chunks are more precise but may lose important context. Larger chunks preserve context but may introduce noise.\n\nHybrid retrieval combines sparse methods like BM25 with dens",
        "start_char": 448,
        "end_char": 960,
        "tier": 1,
        "chunk_index": 1,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.2541346848011017,
        "retrieval_rank": 3,
        "retrieval_method": "dense"
      }
    ],
    "bm25": [
      {
        "chunk_id": "tier1_test_intro_c0001",
        "doc_id": "tier1_test_intro",
        "text": "d a language model for answer generation. Each component introduces potential failure points that must be analyzed systematically.\n\nChunking is the process of splitting long documents into smaller segments suitable for embedding and retrieval. The choice of chunk size and overlap strategy significantly affects retrieval quality. Smaller chunks are more precise but may lose important context. Larger chunks preserve context but may introduce noise.\n\nHybrid retrieval combines sparse methods like BM25 with dens",
        "start_char": 448,
        "end_char": 960,
        "tier": 1,
        "chunk_index": 1,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.7568595573481456,
        "retrieval_rank": 1,
        "retrieval_method": "bm25"
      }
    ],
    "hybrid": [
      {
        "chunk_id": "tier1_test_intro_c0001",
        "doc_id": "tier1_test_intro",
        "text": "d a language model for answer generation. Each component introduces potential failure points that must be analyzed systematically.\n\nChunking is the process of splitting long documents into smaller segments suitable for embedding and retrieval. The choice of chunk size and overlap strategy significantly affects retrieval quality. Smaller chunks are more precise but may lose important context. Larger chunks preserve context but may introduce noise.\n\nHybrid retrieval combines sparse methods like BM25 with dens",
        "start_char": 448,
        "end_char": 960,
        "tier": 1,
        "chunk_index": 1,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.016133229247983348,
        "retrieval_rank": 1,
        "retrieval_method": "hybrid_rrf"
      },
      {
        "chunk_id": "tier1_test_intro_c0000",
        "doc_id": "tier1_test_intro",
        "text": "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with language model generation. Instead of relying solely on parametric knowledge stored in model weights, RAG systems retrieve relevant documents from an external corpus and use them as context for generation.\n\nThe core components of a RAG system include a document corpus, an embedding model for semantic search, a vector database for efficient retrieval, and a language model for answer generation. Each component introdu",
        "start_char": 0,
        "end_char": 512,
        "tier": 1,
        "chunk_index": 0,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.00819672131147541,
        "retrieval_rank": 2,
        "retrieval_method": "hybrid_rrf"
      },
      {
        "chunk_id": "tier1_test_intro_c0002",
        "doc_id": "tier1_test_intro",
        "text": "e.\n\nHybrid retrieval combines sparse methods like BM25 with dense embedding-based search. BM25 excels at exact keyword matching while dense retrieval captures semantic similarity. The combination typically outperforms either method alone, particularly for technical documents.\n\nEvaluation of RAG systems requires metrics beyond simple accuracy. Faithfulness measures how well the generated answer reflects retrieved context. Context precision measures the relevance of retrieved chunks. Answer relevance measures",
        "start_char": 896,
        "end_char": 1408,
        "tier": 1,
        "chunk_index": 2,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.008064516129032258,
        "retrieval_rank": 3,
        "retrieval_method": "hybrid_rrf"
      }
    ]
  },
  {
    "query": "how does hybrid retrieval work?",
    "dense": [
      {
        "chunk_id": "tier1_test_intro_c0002",
        "doc_id": "tier1_test_intro",
        "text": "e.\n\nHybrid retrieval combines sparse methods like BM25 with dense embedding-based search. BM25 excels at exact keyword matching while dense retrieval captures semantic similarity. The combination typically outperforms either method alone, particularly for technical documents.\n\nEvaluation of RAG systems requires metrics beyond simple accuracy. Faithfulness measures how well the generated answer reflects retrieved context. Context precision measures the relevance of retrieved chunks. Answer relevance measures",
        "start_char": 896,
        "end_char": 1408,
        "tier": 1,
        "chunk_index": 2,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.5161415338516235,
        "retrieval_rank": 1,
        "retrieval_method": "dense"
      },
      {
        "chunk_id": "tier1_test_intro_c0003",
        "doc_id": "tier1_test_intro",
        "text": "res the relevance of retrieved chunks. Answer relevance measures how directly the answer addresses the query.",
        "start_char": 1344,
        "end_char": 1856,
        "tier": 1,
        "chunk_index": 3,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.45667025446891785,
        "retrieval_rank": 2,
        "retrieval_method": "dense"
      },
      {
        "chunk_id": "tier1_test_intro_c0000",
        "doc_id": "tier1_test_intro",
        "text": "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with language model generation. Instead of relying solely on parametric knowledge stored in model weights, RAG systems retrieve relevant documents from an external corpus and use them as context for generation.\n\nThe core components of a RAG system include a document corpus, an embedding model for semantic search, a vector database for efficient retrieval, and a language model for answer generation. Each component introdu",
        "start_char": 0,
        "end_char": 512,
        "tier": 1,
        "chunk_index": 0,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.40197139978408813,
        "retrieval_rank": 3,
        "retrieval_method": "dense"
      }
    ],
    "bm25": [
      {
        "chunk_id": "tier1_test_intro_c0000",
        "doc_id": "tier1_test_intro",
        "text": "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with language model generation. Instead of relying solely on parametric knowledge stored in model weights, RAG systems retrieve relevant documents from an external corpus and use them as context for generation.\n\nThe core components of a RAG system include a document corpus, an embedding model for semantic search, a vector database for efficient retrieval, and a language model for answer generation. Each component introdu",
        "start_char": 0,
        "end_char": 512,
        "tier": 1,
        "chunk_index": 0,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.23942883720751532,
        "retrieval_rank": 1,
        "retrieval_method": "bm25"
      },
      {
        "chunk_id": "tier1_test_intro_c0001",
        "doc_id": "tier1_test_intro",
        "text": "d a language model for answer generation. Each component introduces potential failure points that must be analyzed systematically.\n\nChunking is the process of splitting long documents into smaller segments suitable for embedding and retrieval. The choice of chunk size and overlap strategy significantly affects retrieval quality. Smaller chunks are more precise but may lose important context. Larger chunks preserve context but may introduce noise.\n\nHybrid retrieval combines sparse methods like BM25 with dens",
        "start_char": 448,
        "end_char": 960,
        "tier": 1,
        "chunk_index": 1,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.23435484860444214,
        "retrieval_rank": 2,
        "retrieval_method": "bm25"
      },
      {
        "chunk_id": "tier1_test_intro_c0002",
        "doc_id": "tier1_test_intro",
        "text": "e.\n\nHybrid retrieval combines sparse methods like BM25 with dense embedding-based search. BM25 excels at exact keyword matching while dense retrieval captures semantic similarity. The combination typically outperforms either method alone, particularly for technical documents.\n\nEvaluation of RAG systems requires metrics beyond simple accuracy. Faithfulness measures how well the generated answer reflects retrieved context. Context precision measures the relevance of retrieved chunks. Answer relevance measures",
        "start_char": 896,
        "end_char": 1408,
        "tier": 1,
        "chunk_index": 2,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.19605308664415935,
        "retrieval_rank": 3,
        "retrieval_method": "bm25"
      }
    ],
    "hybrid": [
      {
        "chunk_id": "tier1_test_intro_c0002",
        "doc_id": "tier1_test_intro",
        "text": "e.\n\nHybrid retrieval combines sparse methods like BM25 with dense embedding-based search. BM25 excels at exact keyword matching while dense retrieval captures semantic similarity. The combination typically outperforms either method alone, particularly for technical documents.\n\nEvaluation of RAG systems requires metrics beyond simple accuracy. Faithfulness measures how well the generated answer reflects retrieved context. Context precision measures the relevance of retrieved chunks. Answer relevance measures",
        "start_char": 896,
        "end_char": 1408,
        "tier": 1,
        "chunk_index": 2,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.016133229247983348,
        "retrieval_rank": 1,
        "retrieval_method": "hybrid_rrf"
      },
      {
        "chunk_id": "tier1_test_intro_c0000",
        "doc_id": "tier1_test_intro",
        "text": "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with language model generation. Instead of relying solely on parametric knowledge stored in model weights, RAG systems retrieve relevant documents from an external corpus and use them as context for generation.\n\nThe core components of a RAG system include a document corpus, an embedding model for semantic search, a vector database for efficient retrieval, and a language model for answer generation. Each component introdu",
        "start_char": 0,
        "end_char": 512,
        "tier": 1,
        "chunk_index": 0,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.016133229247983348,
        "retrieval_rank": 2,
        "retrieval_method": "hybrid_rrf"
      },
      {
        "chunk_id": "tier1_test_intro_c0001",
        "doc_id": "tier1_test_intro",
        "text": "d a language model for answer generation. Each component introduces potential failure points that must be analyzed systematically.\n\nChunking is the process of splitting long documents into smaller segments suitable for embedding and retrieval. The choice of chunk size and overlap strategy significantly affects retrieval quality. Smaller chunks are more precise but may lose important context. Larger chunks preserve context but may introduce noise.\n\nHybrid retrieval combines sparse methods like BM25 with dens",
        "start_char": 448,
        "end_char": 960,
        "tier": 1,
        "chunk_index": 1,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.015877016129032258,
        "retrieval_rank": 3,
        "retrieval_method": "hybrid_rrf"
      }
    ]
  },
  {
    "query": "what metrics are used to evaluate RAG systems?",
    "dense": [
      {
        "chunk_id": "tier1_test_intro_c0002",
        "doc_id": "tier1_test_intro",
        "text": "e.\n\nHybrid retrieval combines sparse methods like BM25 with dense embedding-based search. BM25 excels at exact keyword matching while dense retrieval captures semantic similarity. The combination typically outperforms either method alone, particularly for technical documents.\n\nEvaluation of RAG systems requires metrics beyond simple accuracy. Faithfulness measures how well the generated answer reflects retrieved context. Context precision measures the relevance of retrieved chunks. Answer relevance measures",
        "start_char": 896,
        "end_char": 1408,
        "tier": 1,
        "chunk_index": 2,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.3271116316318512,
        "retrieval_rank": 1,
        "retrieval_method": "dense"
      },
      {
        "chunk_id": "tier1_test_intro_c0000",
        "doc_id": "tier1_test_intro",
        "text": "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with language model generation. Instead of relying solely on parametric knowledge stored in model weights, RAG systems retrieve relevant documents from an external corpus and use them as context for generation.\n\nThe core components of a RAG system include a document corpus, an embedding model for semantic search, a vector database for efficient retrieval, and a language model for answer generation. Each component introdu",
        "start_char": 0,
        "end_char": 512,
        "tier": 1,
        "chunk_index": 0,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.3013853430747986,
        "retrieval_rank": 2,
        "retrieval_method": "dense"
      },
      {
        "chunk_id": "tier1_test_intro_c0003",
        "doc_id": "tier1_test_intro",
        "text": "res the relevance of retrieved chunks. Answer relevance measures how directly the answer addresses the query.",
        "start_char": 1344,
        "end_char": 1856,
        "tier": 1,
        "chunk_index": 3,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.14746500551700592,
        "retrieval_rank": 3,
        "retrieval_method": "dense"
      }
    ],
    "bm25": [
      {
        "chunk_id": "tier1_test_intro_c0002",
        "doc_id": "tier1_test_intro",
        "text": "e.\n\nHybrid retrieval combines sparse methods like BM25 with dense embedding-based search. BM25 excels at exact keyword matching while dense retrieval captures semantic similarity. The combination typically outperforms either method alone, particularly for technical documents.\n\nEvaluation of RAG systems requires metrics beyond simple accuracy. Faithfulness measures how well the generated answer reflects retrieved context. Context precision measures the relevance of retrieved chunks. Answer relevance measures",
        "start_char": 896,
        "end_char": 1408,
        "tier": 1,
        "chunk_index": 2,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.7500461179721682,
        "retrieval_rank": 1,
        "retrieval_method": "bm25"
      }
    ],
    "hybrid": [
      {
        "chunk_id": "tier1_test_intro_c0002",
        "doc_id": "tier1_test_intro",
        "text": "e.\n\nHybrid retrieval combines sparse methods like BM25 with dense embedding-based search. BM25 excels at exact keyword matching while dense retrieval captures semantic similarity. The combination typically outperforms either method alone, particularly for technical documents.\n\nEvaluation of RAG systems requires metrics beyond simple accuracy. Faithfulness measures how well the generated answer reflects retrieved context. Context precision measures the relevance of retrieved chunks. Answer relevance measures",
        "start_char": 896,
        "end_char": 1408,
        "tier": 1,
        "chunk_index": 2,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.01639344262295082,
        "retrieval_rank": 1,
        "retrieval_method": "hybrid_rrf"
      },
      {
        "chunk_id": "tier1_test_intro_c0000",
        "doc_id": "tier1_test_intro",
        "text": "Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with language model generation. Instead of relying solely on parametric knowledge stored in model weights, RAG systems retrieve relevant documents from an external corpus and use them as context for generation.\n\nThe core components of a RAG system include a document corpus, an embedding model for semantic search, a vector database for efficient retrieval, and a language model for answer generation. Each component introdu",
        "start_char": 0,
        "end_char": 512,
        "tier": 1,
        "chunk_index": 0,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.008064516129032258,
        "retrieval_rank": 2,
        "retrieval_method": "hybrid_rrf"
      },
      {
        "chunk_id": "tier1_test_intro_c0003",
        "doc_id": "tier1_test_intro",
        "text": "res the relevance of retrieved chunks. Answer relevance measures how directly the answer addresses the query.",
        "start_char": 1344,
        "end_char": 1856,
        "tier": 1,
        "chunk_index": 3,
        "total_chunks": 4,
        "metadata": {
          "chunk_size": 512,
          "overlap": 64,
          "strategy": "fixed_size"
        },
        "retrieval_score": 0.007936507936507936,
        "retrieval_rank": 3,
        "retrieval_method": "hybrid_rrf"
      }
    ]
  }
]